# aegis/backends.yaml
backends:
- profile_name: koboldcpp_local
  type: koboldcpp
  llm_url: http://koboldcpp:5001/api/generate
  voice_proxy_url: http://voiceproxy:8001
  rag_url: http://retriever:8000
  api_key: ${BEND_API_KEY}
  model: "NousResearch/Nous-Hermes-2-Mistral-7B-DPO"
  temperature: 0.2
  max_tokens_to_generate: 1536
  top_p: 0.9
  top_k: 40
  repetition_penalty: 1.1

- profile_name: ollama_local
  type: ollama
  llm_url: http://ollama:11434/api
  model: "llama3:instruct"
  temperature: 0.2
  max_tokens_to_generate: 2048
  top_p: 0.9
  top_k: 40
  repetition_penalty: 1.1

- profile_name: vllm_local
  type: vllm
  llm_url: http://vllm:8000/v1/chat/completions
  model: aegis-agent-model
  temperature: 0.2
  max_tokens_to_generate: 2048
  top_p: 0.95
  top_k: -1
  repetition_penalty: 1.1

- profile_name: openai_gpt4
  type: openai
  model: gpt-4-turbo
  api_key: ${OPENAI_API_KEY}
  temperature: 0.7
  max_tokens_to_generate: 2048
  top_p: 1.0
  tts_model: tts-1
  tts_voice: alloy
  stt_model: whisper-1
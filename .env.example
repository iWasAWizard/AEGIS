# .env.example

# --- AEGIS Core Settings ---
AEGIS_HOST=0.0.0.0
AEGIS_PORT=8000
AEGIS_LOG_LEVEL=info
# AEGIS_RELOAD=true # Uncomment for development to enable hot-reloading

# --- LLM Backend Configuration ---
# Default backend is Ollama. Set llm_backend_type = "koboldcpp" in presets/launch to use Kobold.

# OLLAMA_MODEL: Default Ollama model to use if not specified in task/preset (when backend is 'ollama').
# This is also the model the ollama service in docker-compose will try to pull.
OLLAMA_MODEL=llama3:8b-instruct-q5_K_M

# OLLAMA_API_URL: Default URL for the Ollama API (when backend is 'ollama').
OLLAMA_API_URL="http://ollama:11434/api/generate"

# OLLAMA_DIRECT_API_URL: URL for the ollama_generate_direct tool.
# Usually localhost if AEGIS runs on host, or ollama service name if AEGIS is also in Docker.
OLLAMA_DIRECT_API_URL="http://localhost:11434/api/generate"


# KOBOLDCPP_API_URL: URL for the KoboldCPP API /api/v1/generate endpoint.
# Required if using 'koboldcpp' as the llm_backend_type in a preset/launch.
KOBOLDCPP_API_URL="http://localhost:5001/api/v1/generate" # Adjust if KoboldCPP runs elsewhere or in Docker

# KOBOLDCPP_MODEL: Optional. Default model hint for KoboldCPP if llm_model_name not in runtime_config (when backend is 'koboldcpp').
# Also the model the KoboldCPP service in docker-compose will try to load if KOBOLDCPP_MODEL is set there.
# Example: "Llama-3-8B-Instruct.Q5_K_M.gguf"
KOBOLDCPP_MODEL=llama3-8b # A generic hint for Llama3 family for prompt formatter

# Optional KoboldCPP startup parameters (used by its docker-compose.yml service command if defined)
CONTEXT_SIZE=4096
# LAYERS=33 
# THREADS=8 

# --- Machine Manifest Secrets ---
ADMIN_PASSWORD="supersecret_windows_password"
ROOT_PASSWORD="supersecret_linux_root_password"
DEPLOY_PASSWORD="supersecret_deploy_user_password"
ESXI_PASSWORD="supersecret_esxi_password"

# --- OpenAI API Key (Optional) ---
# OPENAI_API_KEY="sk-your_openai_api_key"